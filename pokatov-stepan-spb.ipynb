{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d880fb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:36.166705Z",
     "iopub.status.busy": "2023-01-09T13:31:36.166172Z",
     "iopub.status.idle": "2023-01-09T13:31:36.190338Z",
     "shell.execute_reply": "2023-01-09T13:31:36.188978Z"
    },
    "papermill": {
     "duration": 0.040742,
     "end_time": "2023-01-09T13:31:36.192927",
     "exception": false,
     "start_time": "2023-01-09T13:31:36.152185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/uplift-shift-23/baseline.csv\n",
      "/kaggle/input/uplift-shift-23/x5-uplift-valid/train_purch/train_purch.csv\n",
      "/kaggle/input/uplift-shift-23/x5-uplift-valid/data/products.csv\n",
      "/kaggle/input/uplift-shift-23/x5-uplift-valid/data/clients2.csv\n",
      "/kaggle/input/uplift-shift-23/x5-uplift-valid/data/train.csv\n",
      "/kaggle/input/uplift-shift-23/x5-uplift-valid/data/test.csv\n",
      "/kaggle/input/uplift-shift-23/x5-uplift-valid/test_purch/test_purch.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b7aae",
   "metadata": {
    "papermill": {
     "duration": 0.01206,
     "end_time": "2023-01-09T13:31:36.216179",
     "exception": false,
     "start_time": "2023-01-09T13:31:36.204119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Я **увлекся увеличением score на public leaderboard**: впоследствии оказалось, что в моей модели было намного больше,чем нужно фичей, многие из которых скорее всего привели к неверным предсказаниям модели на скрытой части тестовых данных (хотя на валидационной части тренировочных данных и на public тестовых данных значение score было относительно большим). \n",
    "\n",
    "Наилучшее значение public score было: 0.05036 (private для этой версии: 0.0424)\n",
    "Вторая версия,которую я выбрал имела public score: 0.04957 (private: 0.04472)\n",
    "Разница между этими версиями в том, что в первом случае тренировочные данные делились на данные для обучения и валидационные в пропорции 80% на 20%, а во втором - 70% на 30% (ниже в коде вставлено значение test_size=0.3 - т.е. выбрана вторая версия)\n",
    "\n",
    "Более ранние мои сабмиты, где в модель загружалось меньшее количество признаков, имели public score в районе 0.044, но зато private, как оказалось, у них выше (в одной из версий private: 0.04557) - но, к сожалению, я не отметил эти версии, т.к. на валидационной выборке и на public тестовых данных они давали худший score - кто знал, что эти версии будут лучше работать на приватных тестовых данных..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29676d2f",
   "metadata": {
    "papermill": {
     "duration": 0.012948,
     "end_time": "2023-01-09T13:31:36.241418",
     "exception": false,
     "start_time": "2023-01-09T13:31:36.228470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Функции для работы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97477884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:36.270099Z",
     "iopub.status.busy": "2023-01-09T13:31:36.269079Z",
     "iopub.status.idle": "2023-01-09T13:31:37.257072Z",
     "shell.execute_reply": "2023-01-09T13:31:37.256047Z"
    },
    "papermill": {
     "duration": 1.005924,
     "end_time": "2023-01-09T13:31:37.259562",
     "exception": false,
     "start_time": "2023-01-09T13:31:36.253638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from datetime import datetime\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b804b",
   "metadata": {
    "papermill": {
     "duration": 0.010716,
     "end_time": "2023-01-09T13:31:37.281879",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.271163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Функции загрузки данных и небольшой их предобработки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2384e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.306394Z",
     "iopub.status.busy": "2023-01-09T13:31:37.305633Z",
     "iopub.status.idle": "2023-01-09T13:31:37.321725Z",
     "shell.execute_reply": "2023-01-09T13:31:37.320593Z"
    },
    "papermill": {
     "duration": 0.032168,
     "end_time": "2023-01-09T13:31:37.325103",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.292935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#функция загрузки таблицы clients2.csv\n",
    "def load_clients():\n",
    "    return pd.read_csv(\\\n",
    "        '/kaggle/input/uplift-shift-23/x5-uplift-valid/data/clients2.csv',\n",
    "         parse_dates=['first_issue_date', 'first_redeem_date'])\n",
    "\n",
    "#загрузка + предобработка clients2.csv\n",
    "def prepare_clients():\n",
    "    print('Preparing clients...')\n",
    "    clients = load_clients()\n",
    "    \n",
    "    #есть строковый столбец client_id  -> закодируем числами\n",
    "    client_encoder = LabelEncoder() \n",
    "    clients['client_id'] = client_encoder.fit_transform(clients['client_id'])\n",
    "    print('Clients are ready')\n",
    "    return clients, client_encoder #для client_encoder.classes_\n",
    "\n",
    "\n",
    "#загрузка таблицы products.csv\n",
    "def load_products():\n",
    "    return pd.read_csv(\\\n",
    "        '/kaggle/input/uplift-shift-23/x5-uplift-valid/data/products.csv')\n",
    "\n",
    "#загрузка + предобработка products.csv\n",
    "def prepare_products():\n",
    "    print('Preparing products...')\n",
    "    products = load_products()\n",
    "    product_encoder = LabelEncoder()\n",
    "    products['product_id'] = product_encoder. \\\n",
    "        fit_transform(products['product_id'])\n",
    "    products.fillna(-1, inplace=True) #где N/A проставил \"-1\"\n",
    "    \n",
    "    #строковые столбцы кодируем числами\n",
    "    for col in [\n",
    "        'level_1', 'level_2', 'level_3', 'level_4',\n",
    "        'segment_id', 'brand_id', 'vendor_id']:\n",
    "        products[col] = LabelEncoder().fit_transform(products[col].astype(str))\n",
    "    print('Products are ready')\n",
    "    return products, product_encoder\n",
    "\n",
    "\n",
    "#загрузка таблиц train_purch.csv и test_purch.csv (соединены в одну)\n",
    "def load_purchases():\n",
    "    print('Loading purchases...')\n",
    "    purchases_train = pd.read_csv(\\\n",
    "        '/kaggle/input/uplift-shift-23/x5-uplift-valid/train_purch/train_purch.csv')\n",
    "    purchases_test = pd.read_csv(\\\n",
    "        '/kaggle/input/uplift-shift-23/x5-uplift-valid/test_purch/test_purch.csv')\n",
    "    purchases = pd.concat([purchases_train, purchases_test]) #соединение в одну\n",
    "    print('Purchases are loaded')\n",
    "    return purchases\n",
    "\n",
    "#загрузка + предобработка (train_purch.csv + test_purch.csv)\n",
    "def prepare_purchases(client_encoder,product_encoder):\n",
    "    print('Preparing purchases...')\n",
    "    purchases = load_purchases()\n",
    "\n",
    "    #в ячейки, где N/A проставил \"-1\"\n",
    "    print('Handling N/A values...')\n",
    "    #в 'client_id', 'product_id' ищем пропуски и если есть, то удаляем эти строки\n",
    "    purchases.dropna(subset=['client_id', 'product_id'],\n",
    "        how='any', inplace=True)\n",
    "    #в остальных местах таблицы пропуски заполняем \"-1\"\n",
    "    purchases.fillna(-1, inplace=True)\n",
    "\n",
    "    #кодирование строковых столбцов\n",
    "    print('Label encoding...')\n",
    "    #используем кодировки, созданные в функциях prepare_clients и prepare_products \n",
    "    purchases['client_id'] = client_encoder.transform(purchases['client_id'])\n",
    "    purchases['product_id'] = product_encoder.transform(purchases['product_id'])\n",
    "    for col in ['transaction_id', 'store_id']:\n",
    "        purchases[col] = LabelEncoder(). \\\n",
    "            fit_transform(purchases[col].astype(str))\n",
    "\n",
    "    #переводим дату с временем из строкового типа в формат времени и\n",
    "    #одновременно \"переименовываем\" столбец transaction_datetime в datetime \n",
    "    print('Date and time conversion...')\n",
    "    purchases['datetime'] = pd.to_datetime(\n",
    "        purchases['transaction_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    purchases.drop(columns=['transaction_datetime'], inplace=True)\n",
    "\n",
    "    print('Purchases are ready')\n",
    "    return purchases\n",
    "\n",
    "\n",
    "#загрузка таблицы train.csv\n",
    "def load_train():\n",
    "    return pd.read_csv('/kaggle/input/uplift-shift-23/x5-uplift-valid/data/train.csv',\n",
    "        index_col='client_id')\n",
    "\n",
    "#загрузка таблицы test.csv\n",
    "def load_test():\n",
    "    return pd.read_csv('/kaggle/input/uplift-shift-23/x5-uplift-valid/data/test.csv',\n",
    "        index_col='client_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad71766",
   "metadata": {
    "papermill": {
     "duration": 0.010817,
     "end_time": "2023-01-09T13:31:37.347185",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.336368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Вспомогательные функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbbe4c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.371251Z",
     "iopub.status.busy": "2023-01-09T13:31:37.370809Z",
     "iopub.status.idle": "2023-01-09T13:31:37.379466Z",
     "shell.execute_reply": "2023-01-09T13:31:37.378615Z"
    },
    "papermill": {
     "duration": 0.023135,
     "end_time": "2023-01-09T13:31:37.381647",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.358512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#когда аггрегирующие функции будут применяться, будут появляться мультииндексы\n",
    "#эта функция убирает мультииндекс путем соединения индексов знаком \"_\"\n",
    "def drop_column_multi_index_inplace(df):\n",
    "    df.columns = ['_'.join(t) for t in df.columns]\n",
    "    \n",
    "\n",
    "#эти функции применял для фичей, связанных со временем покупки (утро/день, пн/вт/...)  \n",
    "def make_sum_csr(df,index_col,value_col,col_to_sum):\n",
    "    print(df[col_to_sum].values.shape)\n",
    "    print(df[index_col].values.shape)\n",
    "    print(df[value_col].values.shape)\n",
    "    coo = sparse.coo_matrix((df[col_to_sum].values,(df[index_col].values,df[value_col].values)))\n",
    "    csr = coo.tocsr(copy=False)\n",
    "    return csr\n",
    "        \n",
    "def make_count_csr(df,index_col,value_col):\n",
    "    col_to_sum_name = '__col_to_sum__'\n",
    "    df['__col_to_sum__'] = 1\n",
    "    csr = make_sum_csr(df,index_col=index_col,value_col=value_col,col_to_sum=col_to_sum_name)\n",
    "    df.drop(columns=col_to_sum_name, inplace=True)\n",
    "    return csr    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336d289",
   "metadata": {
    "papermill": {
     "duration": 0.010453,
     "end_time": "2023-01-09T13:31:37.402792",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.392339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **функции для создания фичей**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da834084",
   "metadata": {
    "papermill": {
     "duration": 0.01039,
     "end_time": "2023-01-09T13:31:37.424010",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.413620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**функция создания фичей, связанных с клиентами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aefdeda8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.448603Z",
     "iopub.status.busy": "2023-01-09T13:31:37.447822Z",
     "iopub.status.idle": "2023-01-09T13:31:37.458144Z",
     "shell.execute_reply": "2023-01-09T13:31:37.456905Z"
    },
    "papermill": {
     "duration": 0.025736,
     "end_time": "2023-01-09T13:31:37.461017",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.435281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SECONDS_IN_DAY = 60 * 60 * 24\n",
    "RANDOM_STATE = 1\n",
    "\n",
    "def make_client_features(clients):\n",
    "    print('Preparing features...')\n",
    "    #самая ранняя дата получения карты среди клиентов\n",
    "    min_datetime = clients['first_issue_date'].min() \n",
    "    #число дней с min_datetime до даты получения карты для каждого клиента\n",
    "    days_from_min_to_issue = (\n",
    "        (clients['first_issue_date'] - min_datetime).dt.total_seconds() /\n",
    "            SECONDS_IN_DAY\n",
    "    ).values\n",
    "    #число дней с min_datetime до first_redeem_date\n",
    "    days_from_min_to_redeem = (\n",
    "            (clients['first_redeem_date'] - min_datetime).dt.total_seconds() /\n",
    "            SECONDS_IN_DAY\n",
    "    ).values\n",
    "\n",
    "    #возраст клиентов есть 1852 и -7491?!! => заменяем на числа \"-2\" и \"-3\"\n",
    "    age = clients['age'].values\n",
    "    age[age < 0] = -2\n",
    "    age[age > 100] = -3\n",
    "    \n",
    "    #объединение фичей\n",
    "    print('Combining features')\n",
    "    gender = clients['gender'].values\n",
    "    features = pd.DataFrame({\n",
    "        'client_id': clients['client_id'].values,\n",
    "        'gender_M': (gender == 'M').astype(int),\n",
    "        'gender_F': (gender == 'F').astype(int),\n",
    "        'gender_U': (gender == 'U').astype(int),\n",
    "        'age': age,\n",
    "        'days_from_min_to_issue': days_from_min_to_issue,\n",
    "        'days_from_min_to_redeem': days_from_min_to_redeem,\n",
    "        'issue_redeem_delay': days_from_min_to_redeem - days_from_min_to_issue})\n",
    "    #если вдруг остались пропуски => на \"-1\"\n",
    "    features = features.fillna(-1)\n",
    "    print(f'Client features are created. Shape = {features.shape}')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04022f",
   "metadata": {
    "papermill": {
     "duration": 0.010406,
     "end_time": "2023-01-09T13:31:37.482037",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.471631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**функция создания фичей, связанных с продуктами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbf47c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.506639Z",
     "iopub.status.busy": "2023-01-09T13:31:37.505791Z",
     "iopub.status.idle": "2023-01-09T13:31:37.516420Z",
     "shell.execute_reply": "2023-01-09T13:31:37.514888Z"
    },
    "papermill": {
     "duration": 0.026143,
     "end_time": "2023-01-09T13:31:37.519126",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.492983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_product_features(products,purchases):\n",
    "    #merge таблиц purchases и products (\"inner\")\n",
    "    print('Creating purchases-products matrix')\n",
    "    purchases_products = pd.merge(purchases,products,on='product_id')\n",
    "    print('Purchases-products matrix is ready')\n",
    "\n",
    "    del purchases\n",
    "    del products\n",
    "\n",
    "    print('Creating usual features')\n",
    "    usual_features = make_usual_features(purchases_products)\n",
    "\n",
    "    print(f'Product features are created. Shape = {usual_features.shape}')\n",
    "    return usual_features     \n",
    "\n",
    "\n",
    "#обычные фичи для клиента по покупкам(число уникальных, медиана, макс, мин, сумма)\n",
    "def make_usual_features(purchases_products):\n",
    "    #purchases_products - появляется в make_product_features, где эта функция \n",
    "    #и применяется\n",
    "    pp_gb = purchases_products.groupby('client_id')\n",
    "    usual_features = pp_gb.agg(\n",
    "        {\n",
    "            'netto': ['median', 'max', 'sum'],\n",
    "            'is_own_trademark': ['sum', 'mean'],\n",
    "            'is_alcohol': ['sum', 'mean'],\n",
    "            'level_1': ['nunique'],\n",
    "            'level_2': ['nunique'],\n",
    "            'level_3': ['nunique'],\n",
    "            'level_4': ['nunique'],\n",
    "            'segment_id': ['nunique'],\n",
    "            'brand_id': ['nunique'],\n",
    "            'vendor_id': ['nunique']})\n",
    "    drop_column_multi_index_inplace(usual_features)\n",
    "    usual_features.reset_index(inplace=True)\n",
    "    return usual_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5524944",
   "metadata": {
    "papermill": {
     "duration": 0.010834,
     "end_time": "2023-01-09T13:31:37.540697",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.529863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**фичи, связанные с покупками**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d222e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.564185Z",
     "iopub.status.busy": "2023-01-09T13:31:37.563792Z",
     "iopub.status.idle": "2023-01-09T13:31:37.581228Z",
     "shell.execute_reply": "2023-01-09T13:31:37.579376Z"
    },
    "papermill": {
     "duration": 0.03357,
     "end_time": "2023-01-09T13:31:37.585078",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.551508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ORDER_COLUMNS = [\n",
    "    'transaction_id',\n",
    "    'datetime',\n",
    "    'regular_points_received',\n",
    "    'express_points_received',\n",
    "    'regular_points_spent',\n",
    "    'express_points_spent',\n",
    "    'purchase_sum',\n",
    "    'store_id'\n",
    "]\n",
    "\n",
    "FLOAT32_MAX = np.finfo(np.float32).max\n",
    "TODAY_DATETIME =datetime(2019, 3, 20)\n",
    "POINT_TYPES = ('regular', 'express')\n",
    "POINT_EVENT_TYPES = ('spent', 'received')\n",
    "\n",
    "#фичи по покупкам за последние n_days (пробовал добавить к фичам, но score \n",
    "#становился меньше)\n",
    "def make_purchase_features_for_last_days(purchases,n_days):\n",
    "    print(f'Creating purchase features for last {n_days} days...')\n",
    "    cutoff = TODAY_DATETIME - timedelta(days=n_days)\n",
    "    purchases_last = purchases[purchases['datetime'] >= cutoff]\n",
    "    purchase_last_features = make_purchase_features(purchases_last)\n",
    "    purchase_last_features = purchase_last_features.rename(\n",
    "        columns=lambda x: x+f'_for_last_{n_days}_days' if x!='client_id' else x)\n",
    "    print(f'Purchase features for last {n_days} days are created')\n",
    "    return purchase_last_features\n",
    "\n",
    "\n",
    "#фичи по покупкам\n",
    "def make_purchase_features(purchases):\n",
    "    print('Creating purchase features...')\n",
    "    n_clients = purchases['client_id'].nunique()\n",
    "\n",
    "    print('Creating really purchase features...')\n",
    "    #make_really_purchase_features - ниже\n",
    "    purchase_features = make_really_purchase_features(purchases)\n",
    "    print('Really purchase features are created')\n",
    "\n",
    "    print('Creating small product features...')\n",
    "    #make_small_product_features - ниже\n",
    "    product_features = make_small_product_features(purchases)\n",
    "    print('Small product features are created')\n",
    "\n",
    "    print('Preparing orders table...')\n",
    "\n",
    "    orders = purchases.reindex(columns=['client_id'] + ORDER_COLUMNS)\n",
    "    del purchases\n",
    "    orders.drop_duplicates(inplace=True)\n",
    "    print(f'Orders table is ready. Orders: {len(orders)}')\n",
    "\n",
    "    print('Creating order features...')\n",
    "    #make_order_features - ниже\n",
    "    order_features = make_order_features(orders)\n",
    "    print('Order features are created')\n",
    "\n",
    "    print('Creating store features...')\n",
    "    #make_store_features - ниже\n",
    "    store_features = make_store_features(orders)\n",
    "    print('Store features are created')\n",
    "\n",
    "    print('Creating order interval features...')\n",
    "    #make_order_interval_features - ниже\n",
    "    order_interval_features = make_order_interval_features(orders)\n",
    "    print('Order interval features are created')\n",
    "\n",
    "    print('Creating features for orders with express points spent ...')\n",
    "    #make_features_for_orders_with_express_points_spent - ниже\n",
    "    orders_with_express_points_spent_features = \\\n",
    "        make_features_for_orders_with_express_points_spent(orders)\n",
    "    print('Features for orders with express points spent are created')\n",
    "\n",
    "    #соединение фичей\n",
    "    features = (\n",
    "        purchase_features\n",
    "        .merge(order_features, on='client_id')\n",
    "        .merge(product_features, on='client_id')\n",
    "        .merge(store_features, on='client_id')\n",
    "        .merge(order_interval_features, on='client_id')\n",
    "        .merge(orders_with_express_points_spent_features, on='client_id')\n",
    "    )\n",
    "\n",
    "    #проверка на равенство числа строк в таблицах\n",
    "    assert len(features) == n_clients, \\\n",
    "        f'n_clients = {n_clients} but len(features) = {len(features)}'\n",
    "\n",
    "    #добавление еще фич\n",
    "    features['days_from_last_order_share'] = \\\n",
    "        features['days_from_last_order'] / features['orders_interval_median']\n",
    "\n",
    "    features['most_popular_store_share'] = (\n",
    "        features['store_transaction_id_count_max'] /\n",
    "        features['transaction_id_count']\n",
    "    )\n",
    "\n",
    "    features['ratio_days_from_last_order_eps_to_median_interval_eps'] = (\n",
    "        features['days_from_last_express_points_spent'] /\n",
    "        features['orders_interval_median_eps']\n",
    "    )\n",
    "\n",
    "    features['ratio_mean_purchase_sum_eps_to_mean_purchase_sum'] = (\n",
    "        features['median_purchase_sum_eps'] /\n",
    "        features['purchase_sum_median']\n",
    "    )\n",
    "\n",
    "    print(f'Purchase features are created. Shape = {features.shape}')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78096bb",
   "metadata": {
    "papermill": {
     "duration": 0.011125,
     "end_time": "2023-01-09T13:31:37.609389",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.598264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**функции, которые вызываются в функции make_purchase_features(purchases)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f2cc8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.637616Z",
     "iopub.status.busy": "2023-01-09T13:31:37.636670Z",
     "iopub.status.idle": "2023-01-09T13:31:37.681893Z",
     "shell.execute_reply": "2023-01-09T13:31:37.680595Z"
    },
    "papermill": {
     "duration": 0.061777,
     "end_time": "2023-01-09T13:31:37.685079",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.623302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_really_purchase_features(purchases):\n",
    "    simple_purchases = purchases.reindex(\n",
    "        columns=['client_id', 'product_id', 'trn_sum_from_iss']\n",
    "    )\n",
    "    prices_bounds = [0, 98, 195, 490, 950, 1900, 4400, FLOAT32_MAX]\n",
    "    agg_dict = {}\n",
    "    #принадлежность trn_sum_from_iss промежуткам вида [lower_bound, upper_bound)\n",
    "    for i, lower_bound in enumerate(prices_bounds[:-1]):\n",
    "        upper_bound = prices_bounds[i + 1]\n",
    "        name = f'price_from_{lower_bound}'\n",
    "        simple_purchases[name] = (\n",
    "            (simple_purchases['trn_sum_from_iss'] >= lower_bound) &\n",
    "            (simple_purchases['trn_sum_from_iss'] < upper_bound)\n",
    "        ).astype(int)\n",
    "        agg_dict[name] = ['sum', 'mean']\n",
    "\n",
    "    agg_dict.update(\n",
    "        {\n",
    "            'trn_sum_from_iss': ['median'],  \n",
    "            'product_id': ['count', 'nunique']\n",
    "        }\n",
    "    )\n",
    "    simple_features = simple_purchases.groupby('client_id').agg(agg_dict)\n",
    "    drop_column_multi_index_inplace(simple_features)\n",
    "    simple_features.reset_index(inplace=True)\n",
    "\n",
    "    p_gb = purchases.groupby(['client_id', 'transaction_id'])\n",
    "    purchase_agg = p_gb.agg(\n",
    "        {\n",
    "            'product_id': ['count'],\n",
    "            'product_quantity': ['max']\n",
    "        }\n",
    "    )\n",
    "    drop_column_multi_index_inplace(purchase_agg)\n",
    "    purchase_agg.reset_index(inplace=True)\n",
    "    o_gb = purchase_agg.groupby('client_id')\n",
    "    complex_features = o_gb.agg(\n",
    "        {\n",
    "            'product_id_count': ['mean', 'median'],\n",
    "            'product_quantity_max': ['mean', 'median']\n",
    "        }\n",
    "    )\n",
    "    drop_column_multi_index_inplace(complex_features)\n",
    "    complex_features.reset_index(inplace=True)\n",
    "    features = pd.merge(\n",
    "        simple_features,\n",
    "        complex_features,\n",
    "        on='client_id'\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def make_small_product_features(purchases):\n",
    "    cl_pr_gb = purchases.groupby(['client_id', 'product_id'])\n",
    "    product_agg = cl_pr_gb.agg({\n",
    "        'product_quantity': ['sum']})\n",
    "\n",
    "    drop_column_multi_index_inplace(product_agg)\n",
    "    product_agg.reset_index(inplace=True)\n",
    "\n",
    "    cl_gb = product_agg.groupby(['client_id'])\n",
    "    features = cl_gb.agg({'product_quantity_sum': ['max']})\n",
    "\n",
    "    drop_column_multi_index_inplace(features)\n",
    "    features.reset_index(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def make_order_features(orders):\n",
    "    o_gb = orders.groupby('client_id')\n",
    "\n",
    "    agg_dict = {\n",
    "            'transaction_id': ['count'],  \n",
    "            'regular_points_received': ['sum', 'max', 'median'],\n",
    "            'express_points_received': ['sum', 'max', 'median'],\n",
    "            'regular_points_spent': ['sum', 'min', 'median'],\n",
    "            'express_points_spent': ['sum', 'min', 'median'],\n",
    "            'purchase_sum': ['sum', 'max', 'median'],\n",
    "            'store_id': ['nunique'],  \n",
    "            'datetime': ['max'] \n",
    "        }\n",
    "\n",
    "    #  regular/express points потрачены/получены?\n",
    "    for points_type in POINT_TYPES:\n",
    "        for event_type in POINT_EVENT_TYPES:\n",
    "            col_name = f'{points_type}_points_{event_type}'\n",
    "            new_col_name = f'is_{points_type}_points_{event_type}'\n",
    "            orders[new_col_name] = (orders[col_name] != 0).astype(int)\n",
    "            agg_dict[new_col_name] = ['sum']\n",
    "\n",
    "    features = o_gb.agg(agg_dict)\n",
    "    drop_column_multi_index_inplace(features)\n",
    "    features.reset_index(inplace=True)\n",
    "\n",
    "    features['days_from_last_order'] = (\n",
    "        TODAY_DATETIME - features['datetime_max']\n",
    "    ).dt.total_seconds() // SECONDS_IN_DAY\n",
    "    features.drop(columns=['datetime_max'], inplace=True)\n",
    "\n",
    "    # отношение потраченных regular/express points ко всем транзакциям\n",
    "    for points_type in POINT_TYPES:\n",
    "        for event_type in POINT_EVENT_TYPES:\n",
    "            col_name = f'is_{points_type}_points_{event_type}_sum'\n",
    "            new_col_name = f'proportion_count_{points_type}_points_{event_type}'\n",
    "            features[new_col_name] = (\n",
    "                    features[col_name] / features['transaction_id_count']\n",
    "            )\n",
    "\n",
    "    express_col = f'is_express_points_spent_sum'\n",
    "    regular_col = f'is_regular_points_spent_sum'\n",
    "    new_col_name = f'ratio_count_express_to_regular_points_spent'\n",
    "    features[new_col_name] = (\n",
    "            features[express_col] / features[regular_col]\n",
    "    ).replace(np.inf, FLOAT32_MAX) #если деление на ноль, то замена inf на число\n",
    "\n",
    "    for points_type in POINT_TYPES:\n",
    "        spent_col = f'is_{points_type}_points_spent_sum'\n",
    "        received_col = f'is_{points_type}_points_received_sum'\n",
    "        new_col_name = f'ratio_count_{points_type}_points_spent_to_received'\n",
    "        features[new_col_name] = (\n",
    "                features[spent_col] / features[received_col]\n",
    "        ).replace(np.inf, 1000)\n",
    "\n",
    "\n",
    "    for points_type in POINT_TYPES:\n",
    "        spent_col = f'{points_type}_points_spent_sum'\n",
    "        orders_sum_col = f'purchase_sum_sum'\n",
    "        new_col_name = f'ratio_sum_{points_type}_points_spent_to_purchases_sum'\n",
    "        features[new_col_name] = features[spent_col] / features[orders_sum_col]\n",
    "\n",
    "    new_col_name = f'ratio_sum_express_points_spent_to_sum_regular_points_spent'\n",
    "    regular_col = f'regular_points_spent_sum'\n",
    "    express_col = f'express_points_spent_sum'\n",
    "    features[new_col_name] = features[express_col] / features[regular_col]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def make_store_features(orders):\n",
    "    cl_st_gb = orders.groupby(['client_id', 'store_id'])\n",
    "    store_agg = cl_st_gb.agg({\n",
    "        'transaction_id': ['count']})\n",
    "\n",
    "    drop_column_multi_index_inplace(store_agg)\n",
    "    store_agg.reset_index(inplace=True)\n",
    "\n",
    "    cl_gb = store_agg.groupby(['client_id'])\n",
    "    simple_features = cl_gb.agg(\n",
    "        {\n",
    "            'transaction_id_count': ['max', 'mean', 'median']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    drop_column_multi_index_inplace(simple_features)\n",
    "    simple_features.reset_index(inplace=True)\n",
    "    simple_features.columns = (\n",
    "        ['client_id'] +\n",
    "        [\n",
    "            f'store_{col}'\n",
    "            for col in simple_features.columns[1:]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return simple_features  \n",
    "\n",
    "\n",
    "\n",
    "def make_order_interval_features(orders):\n",
    "    orders = orders.sort_values(['client_id', 'datetime'])\n",
    "\n",
    "    last_order_client = orders['client_id'].shift(1)\n",
    "    is_same_client = last_order_client == orders['client_id']\n",
    "    orders['last_order_datetime'] = orders['datetime'].shift(1)\n",
    "\n",
    "    orders['orders_interval'] = np.nan\n",
    "    orders.loc[is_same_client, 'orders_interval'] = (\n",
    "        orders.loc[is_same_client, 'datetime'] -\n",
    "        orders.loc[is_same_client, 'last_order_datetime']\n",
    "    ).dt.total_seconds() / SECONDS_IN_DAY\n",
    "\n",
    "    cl_gb = orders.groupby('client_id', sort=False)\n",
    "    features = cl_gb.agg(\n",
    "        {'orders_interval': ['mean',  'median','std',  'min','max','last']}\n",
    "    )\n",
    "    drop_column_multi_index_inplace(features)\n",
    "    features.reset_index(inplace=True)\n",
    "    features.fillna(-3, inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def make_features_for_orders_with_express_points_spent(orders):\n",
    "    orders_with_eps = orders.loc[orders['express_points_spent'] != 0]\n",
    "\n",
    "    o_gb = orders_with_eps.groupby(['client_id'])\n",
    "    features = o_gb.agg(\n",
    "        {'purchase_sum': ['median'], 'datetime': ['max']}\n",
    "    )\n",
    "    drop_column_multi_index_inplace(features)\n",
    "    features.reset_index(inplace=True)\n",
    "    features['days_from_last_express_points_spent'] = (\n",
    "            TODAY_DATETIME - features['datetime_max']\n",
    "    ).dt.days\n",
    "    features.drop(columns=['datetime_max'], inplace=True)\n",
    "    features.rename(\n",
    "        columns={\n",
    "            'purchase_sum_median': 'median_purchase_sum_eps'\n",
    "        },\n",
    "        inplace=True)\n",
    "\n",
    "    # make_order_interval_features - выше\n",
    "    order_int_features = make_order_interval_features(orders_with_eps)\n",
    "    renamings = {\n",
    "        col: f'{col}_eps'\n",
    "        for col in order_int_features\n",
    "        if col != 'client_id'\n",
    "    }\n",
    "    order_int_features.rename(columns=renamings, inplace=True)\n",
    "\n",
    "    features = pd.merge(\n",
    "        features,\n",
    "        order_int_features,\n",
    "        on='client_id')\n",
    "\n",
    "    features = features.merge(\n",
    "        pd.Series(orders['client_id'].unique(), name='client_id'),\n",
    "        how='right')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565e469",
   "metadata": {
    "papermill": {
     "duration": 0.011442,
     "end_time": "2023-01-09T13:31:37.709035",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.697593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**функция для создания фичей, связанных со временем покупки(утро/день..., пн/вт...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c382af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.734330Z",
     "iopub.status.busy": "2023-01-09T13:31:37.733870Z",
     "iopub.status.idle": "2023-01-09T13:31:37.747139Z",
     "shell.execute_reply": "2023-01-09T13:31:37.745600Z"
    },
    "papermill": {
     "duration": 0.028814,
     "end_time": "2023-01-09T13:31:37.749806",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.720992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEEK_DAYS = [\n",
    "    'Monday',\n",
    "    'Tuesday',\n",
    "    'Wednesday',\n",
    "    'Thursday',\n",
    "    'Friday',\n",
    "    'Saturday',\n",
    "    'Sunday'\n",
    "]\n",
    "TIME_LABELS = ['Night', 'Morning', 'Afternoon', 'Evening']\n",
    "\n",
    "def make_time_features(orders):\n",
    "    # np.unique возвращает отсортированный массив\n",
    "    client_ids = np.unique(orders['client_id'].values)\n",
    "\n",
    "    orders['weekday'] = np.array(WEEK_DAYS)[\n",
    "        orders['datetime'].dt.dayofweek.values\n",
    "    ]\n",
    "\n",
    "    time_bins = [-1, 6, 11, 18, 24] #утро/день/вечер/ночь\n",
    "\n",
    "    orders['part_of_day'] = pd.cut(\n",
    "        orders['datetime'].dt.hour,\n",
    "        bins=time_bins,\n",
    "        labels=TIME_LABELS\n",
    "    ).astype(str)\n",
    "\n",
    "    time_part_encoder = LabelEncoder()\n",
    "    orders['part_of_day'] = time_part_encoder.fit_transform(orders['part_of_day'])\n",
    "\n",
    "    time_part_columns_name = time_part_encoder.inverse_transform(\n",
    "        np.arange(len(time_part_encoder.classes_))\n",
    "    )\n",
    "\n",
    "    time_part_cols = make_count_csr(orders,index_col='client_id',value_col='part_of_day')[client_ids, :]  # drop empty rows\n",
    "\n",
    "    time_part_cols = pd.DataFrame(\n",
    "        time_part_cols.toarray(),\n",
    "        columns=time_part_columns_name)\n",
    "    time_part_cols['client_id'] = client_ids\n",
    "\n",
    "    weekday_encoder = LabelEncoder()\n",
    "    orders['weekday'] = weekday_encoder.fit_transform(orders['weekday'])\n",
    "\n",
    "    weekday_column_names = weekday_encoder.inverse_transform(\n",
    "        np.arange(len(weekday_encoder.classes_))\n",
    "    )\n",
    "    weekday_cols = make_count_csr(\n",
    "        orders,\n",
    "        index_col='client_id',\n",
    "        value_col='weekday')[client_ids, :]  \n",
    "    weekday_cols = pd.DataFrame(\n",
    "        weekday_cols.toarray(),\n",
    "        columns=weekday_column_names)\n",
    "    weekday_cols['client_id'] = client_ids\n",
    "\n",
    "    time_part_features = pd.merge(\n",
    "        left=time_part_cols,\n",
    "        right=weekday_cols,\n",
    "        on='client_id')\n",
    "    time_part_features.columns = [\n",
    "        f'{col}_orders_count' if col != 'client_id' else col\n",
    "        for col in time_part_features.columns\n",
    "    ]\n",
    "\n",
    "    return time_part_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb22fdea",
   "metadata": {
    "papermill": {
     "duration": 0.010516,
     "end_time": "2023-01-09T13:31:37.771345",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.760829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**функции для обучения модели и проверки ее качества**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4850f4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.795639Z",
     "iopub.status.busy": "2023-01-09T13:31:37.794827Z",
     "iopub.status.idle": "2023-01-09T13:31:37.807867Z",
     "shell.execute_reply": "2023-01-09T13:31:37.806810Z"
    },
    "papermill": {
     "duration": 0.027911,
     "end_time": "2023-01-09T13:31:37.810379",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.782468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#оценка качества модели\n",
    "def uplift_metrics(prediction,treatment,target,rate_for_uplift = 0.3):\n",
    "    scores = {\n",
    "        'roc_auc': score_roc_auc(prediction, treatment, target),\n",
    "        'uplift': score_uplift(prediction, treatment, target, rate_for_uplift)\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "#функции, которые вызываются в функции uplift_metrics\n",
    "def score_uplift(prediction,treatment,target,rate = 0.3):\n",
    "    order = np.argsort(-prediction)\n",
    "    treatment_n = int((treatment == 1).sum() * rate)\n",
    "    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n",
    "    control_n = int((treatment == 0).sum() * rate)\n",
    "    control_p = target[order][treatment[order] == 0][:control_n].mean()\n",
    "    score = treatment_p - control_p\n",
    "    return score\n",
    "\n",
    "\n",
    "def score_roc_auc(prediction,treatment,target):\n",
    "    y_true = make_z(treatment, target)\n",
    "    score = roc_auc_score(y_true, prediction)\n",
    "    return score\n",
    "\n",
    "\n",
    "#функция для score_roc_auc\n",
    "def uplift_fit(model, X_train, treatment_train, target_train):\n",
    "    z = make_z(treatment_train, target_train)\n",
    "    model = clone(model)\n",
    "    model.fit(X_train, z)\n",
    "    return model\n",
    "\n",
    "#функция для предсказания вероятностей/подсчета аплифта\n",
    "def uplift_predict(model, X_test, z=True):\n",
    "    predict_z = model.predict_proba(X_test)[:, 1]\n",
    "    uplift = calc_uplift(predict_z)\n",
    "    if z: return predict_z\n",
    "    else: return uplift\n",
    "    \n",
    "\n",
    "#функция для uplift_fit   \n",
    "def make_z(treatment, target):\n",
    "    y = target\n",
    "    w = treatment\n",
    "    z = y * w + (1 - y) * (1 - w) \n",
    "    return z \n",
    "\n",
    "#функция для uplift_predict\n",
    "def calc_uplift(prediction):\n",
    "    uplift = 2 * prediction - 1\n",
    "    return uplift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7378ab",
   "metadata": {
    "papermill": {
     "duration": 0.010519,
     "end_time": "2023-01-09T13:31:37.831546",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.821027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Функция создания таблицы с фичами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1216782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.857057Z",
     "iopub.status.busy": "2023-01-09T13:31:37.855710Z",
     "iopub.status.idle": "2023-01-09T13:31:37.867081Z",
     "shell.execute_reply": "2023-01-09T13:31:37.865901Z"
    },
    "papermill": {
     "duration": 0.027045,
     "end_time": "2023-01-09T13:31:37.869696",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.842651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_features():\n",
    "    print('Loading data...')\n",
    "    clients, client_encoder = prepare_clients()\n",
    "    products, product_encoder = prepare_products()\n",
    "    purchases = prepare_purchases(client_encoder, product_encoder)\n",
    "    \n",
    "    del product_encoder\n",
    "    print('Data is loaded')\n",
    "\n",
    "    print('Preparing features...')\n",
    "    purchase_features = make_purchase_features(purchases)\n",
    "\n",
    "    purchases_ids = purchases.reindex(columns=['client_id', 'product_id'])\n",
    "    \n",
    "    orders = purchases.reindex(columns=['client_id'] + ORDER_COLUMNS)\n",
    "    orders.drop_duplicates(inplace=True)\n",
    "    time_features=make_time_features(orders)\n",
    "    del purchases\n",
    "    product_features = make_product_features(products, purchases_ids)\n",
    "    del purchases_ids\n",
    "\n",
    "    client_features = make_client_features(clients)\n",
    "\n",
    "    print('Combining features...')\n",
    "    features = (\n",
    "        client_features\n",
    "            .merge(purchase_features, on='client_id', how='left')\n",
    "            .merge(product_features, on='client_id', how='left')\n",
    "            .merge(time_features, on='client_id', how='left')\n",
    "    )\n",
    "    del client_features\n",
    "    del purchase_features\n",
    "    del product_features\n",
    "\n",
    "    features.fillna(-2, inplace=True)\n",
    "\n",
    "    features['client_id'] = client_encoder.inverse_transform(features['client_id'])\n",
    "    del client_encoder\n",
    "\n",
    "    print('Features are ready')\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85b750",
   "metadata": {
    "papermill": {
     "duration": 0.010541,
     "end_time": "2023-01-09T13:31:37.891764",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.881223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Функция сохранения результатов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2597feae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.916959Z",
     "iopub.status.busy": "2023-01-09T13:31:37.915807Z",
     "iopub.status.idle": "2023-01-09T13:31:37.921905Z",
     "shell.execute_reply": "2023-01-09T13:31:37.921092Z"
    },
    "papermill": {
     "duration": 0.021748,
     "end_time": "2023-01-09T13:31:37.924250",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.902502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_submission(indices_test, test_pred, filename):\n",
    "    df_submission = pd.DataFrame({'pred': test_pred}, index=indices_test)\n",
    "    df_submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267afb6",
   "metadata": {
    "papermill": {
     "duration": 0.010948,
     "end_time": "2023-01-09T13:31:37.945828",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.934880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Создание таблицы фичей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5a4c579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:31:37.970120Z",
     "iopub.status.busy": "2023-01-09T13:31:37.969485Z",
     "iopub.status.idle": "2023-01-09T13:35:51.922695Z",
     "shell.execute_reply": "2023-01-09T13:35:51.921276Z"
    },
    "papermill": {
     "duration": 253.968719,
     "end_time": "2023-01-09T13:35:51.925531",
     "exception": false,
     "start_time": "2023-01-09T13:31:37.956812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preparing clients...\n",
      "Clients are ready\n",
      "Preparing products...\n",
      "Products are ready\n",
      "Preparing purchases...\n",
      "Loading purchases...\n",
      "Purchases are loaded\n",
      "Handling N/A values...\n",
      "Label encoding...\n",
      "Date and time conversion...\n",
      "Purchases are ready\n",
      "Data is loaded\n",
      "Preparing features...\n",
      "Creating purchase features...\n",
      "Creating really purchase features...\n",
      "Really purchase features are created\n",
      "Creating small product features...\n",
      "Small product features are created\n",
      "Preparing orders table...\n",
      "Orders table is ready. Orders: 4024949\n",
      "Creating order features...\n",
      "Order features are created\n",
      "Creating store features...\n",
      "Store features are created\n",
      "Creating order interval features...\n",
      "Order interval features are created\n",
      "Creating features for orders with express points spent ...\n",
      "Features for orders with express points spent are created\n",
      "Purchase features are created. Shape = (200039, 76)\n",
      "(4024949,)\n",
      "(4024949,)\n",
      "(4024949,)\n",
      "(4024949,)\n",
      "(4024949,)\n",
      "(4024949,)\n",
      "Creating purchases-products matrix\n",
      "Purchases-products matrix is ready\n",
      "Creating usual features\n",
      "Product features are created. Shape = (200039, 15)\n",
      "Preparing features...\n",
      "Combining features\n",
      "Client features are created. Shape = (200039, 8)\n",
      "Combining features...\n",
      "Features are ready\n",
      "Features shape: (200039, 108)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_M</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_U</th>\n",
       "      <th>age</th>\n",
       "      <th>days_from_min_to_issue</th>\n",
       "      <th>days_from_min_to_redeem</th>\n",
       "      <th>issue_redeem_delay</th>\n",
       "      <th>price_from_0_sum</th>\n",
       "      <th>price_from_0_mean</th>\n",
       "      <th>price_from_98_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>Evening_orders_count</th>\n",
       "      <th>Morning_orders_count</th>\n",
       "      <th>Night_orders_count</th>\n",
       "      <th>Friday_orders_count</th>\n",
       "      <th>Monday_orders_count</th>\n",
       "      <th>Saturday_orders_count</th>\n",
       "      <th>Sunday_orders_count</th>\n",
       "      <th>Thursday_orders_count</th>\n",
       "      <th>Tuesday_orders_count</th>\n",
       "      <th>Wednesday_orders_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000012768d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>122.886458</td>\n",
       "      <td>275.045706</td>\n",
       "      <td>152.159248</td>\n",
       "      <td>48</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000036f903</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>5.812558</td>\n",
       "      <td>18.759468</td>\n",
       "      <td>12.946910</td>\n",
       "      <td>141</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00010925a5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>475.914711</td>\n",
       "      <td>527.908692</td>\n",
       "      <td>51.993981</td>\n",
       "      <td>61</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001f552b0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>87.039120</td>\n",
       "      <td>510.774618</td>\n",
       "      <td>423.735498</td>\n",
       "      <td>70</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00020e7b18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>236.720451</td>\n",
       "      <td>280.976238</td>\n",
       "      <td>44.255787</td>\n",
       "      <td>186</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffe0abb97</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>236.605972</td>\n",
       "      <td>312.626273</td>\n",
       "      <td>76.020301</td>\n",
       "      <td>27</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffe0ed719</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>163.603542</td>\n",
       "      <td>251.851319</td>\n",
       "      <td>88.247778</td>\n",
       "      <td>143</td>\n",
       "      <td>0.831395</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffea1204c</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>301.941192</td>\n",
       "      <td>341.943160</td>\n",
       "      <td>40.001968</td>\n",
       "      <td>46</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffeca6d22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>267.730498</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffff6ce77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>121.083958</td>\n",
       "      <td>143.928738</td>\n",
       "      <td>22.844780</td>\n",
       "      <td>128</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200039 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender_M  gender_F  gender_U  age  days_from_min_to_issue  \\\n",
       "client_id                                                               \n",
       "000012768d         0         0         1   45              122.886458   \n",
       "000036f903         0         1         0   72                5.812558   \n",
       "00010925a5         0         0         1   83              475.914711   \n",
       "0001f552b0         0         1         0   33               87.039120   \n",
       "00020e7b18         0         0         1   73              236.720451   \n",
       "...              ...       ...       ...  ...                     ...   \n",
       "fffe0abb97         0         1         0   35              236.605972   \n",
       "fffe0ed719         0         0         1   69              163.603542   \n",
       "fffea1204c         0         1         0   73              301.941192   \n",
       "fffeca6d22         0         1         0   77              267.730498   \n",
       "fffff6ce77         0         0         1   42              121.083958   \n",
       "\n",
       "            days_from_min_to_redeem  issue_redeem_delay  price_from_0_sum  \\\n",
       "client_id                                                                   \n",
       "000012768d               275.045706          152.159248                48   \n",
       "000036f903                18.759468           12.946910               141   \n",
       "00010925a5               527.908692           51.993981                61   \n",
       "0001f552b0               510.774618          423.735498                70   \n",
       "00020e7b18               280.976238           44.255787               186   \n",
       "...                             ...                 ...               ...   \n",
       "fffe0abb97               312.626273           76.020301                27   \n",
       "fffe0ed719               251.851319           88.247778               143   \n",
       "fffea1204c               341.943160           40.001968                46   \n",
       "fffeca6d22                -1.000000           -1.000000                50   \n",
       "fffff6ce77               143.928738           22.844780               128   \n",
       "\n",
       "            price_from_0_mean  price_from_98_sum  ...  Evening_orders_count  \\\n",
       "client_id                                         ...                         \n",
       "000012768d           0.923077                  4  ...                     0   \n",
       "000036f903           0.870370                 19  ...                     0   \n",
       "00010925a5           0.782051                 14  ...                     0   \n",
       "0001f552b0           0.813953                 11  ...                     0   \n",
       "00020e7b18           0.683824                 60  ...                     0   \n",
       "...                       ...                ...  ...                   ...   \n",
       "fffe0abb97           0.710526                 10  ...                     0   \n",
       "fffe0ed719           0.831395                 28  ...                     0   \n",
       "fffea1204c           0.754098                 11  ...                     0   \n",
       "fffeca6d22           0.925926                  3  ...                     0   \n",
       "fffff6ce77           0.630542                 54  ...                     2   \n",
       "\n",
       "            Morning_orders_count  Night_orders_count  Friday_orders_count  \\\n",
       "client_id                                                                   \n",
       "000012768d                     3                   0                    1   \n",
       "000036f903                    31                   0                    3   \n",
       "00010925a5                    13                   1                    1   \n",
       "0001f552b0                     8                   0                    1   \n",
       "00020e7b18                    15                   1                    6   \n",
       "...                          ...                 ...                  ...   \n",
       "fffe0abb97                     5                   3                    5   \n",
       "fffe0ed719                     4                   0                    4   \n",
       "fffea1204c                     1                   0                    4   \n",
       "fffeca6d22                    15                   0                    4   \n",
       "fffff6ce77                     0                   3                    2   \n",
       "\n",
       "            Monday_orders_count  Saturday_orders_count  Sunday_orders_count  \\\n",
       "client_id                                                                     \n",
       "000012768d                    0                      1                    1   \n",
       "000036f903                    3                      1                    9   \n",
       "00010925a5                    5                      1                    2   \n",
       "0001f552b0                    3                      4                    2   \n",
       "00020e7b18                    0                      0                    0   \n",
       "...                         ...                    ...                  ...   \n",
       "fffe0abb97                    0                      1                    2   \n",
       "fffe0ed719                    4                      4                    4   \n",
       "fffea1204c                    0                      2                    1   \n",
       "fffeca6d22                    1                      1                    3   \n",
       "fffff6ce77                    4                      3                    5   \n",
       "\n",
       "            Thursday_orders_count  Tuesday_orders_count  \\\n",
       "client_id                                                 \n",
       "000012768d                      1                     0   \n",
       "000036f903                      3                     7   \n",
       "00010925a5                      2                     6   \n",
       "0001f552b0                      3                     0   \n",
       "00020e7b18                      3                     3   \n",
       "...                           ...                   ...   \n",
       "fffe0abb97                      0                     1   \n",
       "fffe0ed719                      8                     2   \n",
       "fffea1204c                      2                     2   \n",
       "fffeca6d22                      2                     2   \n",
       "fffff6ce77                      7                     4   \n",
       "\n",
       "            Wednesday_orders_count  \n",
       "client_id                           \n",
       "000012768d                       0  \n",
       "000036f903                       6  \n",
       "00010925a5                       1  \n",
       "0001f552b0                       2  \n",
       "00020e7b18                       6  \n",
       "...                            ...  \n",
       "fffe0abb97                       0  \n",
       "fffe0ed719                       4  \n",
       "fffea1204c                       6  \n",
       "fffeca6d22                       3  \n",
       "fffff6ce77                       7  \n",
       "\n",
       "[200039 rows x 107 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = prepare_features()\n",
    "print(f'Features shape: {features.shape}')\n",
    "features.set_index('client_id', inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ba2b4",
   "metadata": {
    "papermill": {
     "duration": 0.01272,
     "end_time": "2023-01-09T13:35:51.951380",
     "exception": false,
     "start_time": "2023-01-09T13:35:51.938660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Подготовка датасетов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bead329f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:35:51.978947Z",
     "iopub.status.busy": "2023-01-09T13:35:51.978489Z",
     "iopub.status.idle": "2023-01-09T13:35:53.233634Z",
     "shell.execute_reply": "2023-01-09T13:35:53.231977Z"
    },
    "papermill": {
     "duration": 1.272042,
     "end_time": "2023-01-09T13:35:53.236121",
     "exception": false,
     "start_time": "2023-01-09T13:35:51.964079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sets prepared\n"
     ]
    }
   ],
   "source": [
    "train = load_train()\n",
    "test = load_test()\n",
    "indices_train = train.index #клиенты в тренировочном наборе\n",
    "indices_test = test.index  #клиенты в тестовом наборе\n",
    "\n",
    "X_train = features.loc[indices_train]\n",
    "treatment_train = train.loc[indices_train, 'treatment_flg'].values\n",
    "target_train = train.loc[indices_train, 'purchased'].values\n",
    "\n",
    "X_test = features.loc[indices_test]\n",
    "\n",
    "indices_learn, indices_valid = train_test_split(train.index,test_size=0.2,random_state=RANDOM_STATE + 1)\n",
    "\n",
    "X_learn = features.loc[indices_learn]\n",
    "treatment_learn = train.loc[indices_learn, 'treatment_flg'].values\n",
    "target_learn = train.loc[indices_learn, 'purchased'].values\n",
    "\n",
    "X_valid = features.loc[indices_valid]\n",
    "treatment_valid = train.loc[indices_valid, 'treatment_flg'].values\n",
    "target_valid = train.loc[indices_valid, 'purchased'].values\n",
    "print('Data sets prepared')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a98e4",
   "metadata": {
    "papermill": {
     "duration": 0.012629,
     "end_time": "2023-01-09T13:35:53.261825",
     "exception": false,
     "start_time": "2023-01-09T13:35:53.249196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Модель**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e492c2",
   "metadata": {
    "papermill": {
     "duration": 0.012532,
     "end_time": "2023-01-09T13:35:53.287252",
     "exception": false,
     "start_time": "2023-01-09T13:35:53.274720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Как подбирались гиперпараметры**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709737fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T09:43:13.381008Z",
     "iopub.status.busy": "2023-01-05T09:43:13.380525Z",
     "iopub.status.idle": "2023-01-05T09:43:13.411172Z",
     "shell.execute_reply": "2023-01-05T09:43:13.410187Z",
     "shell.execute_reply.started": "2023-01-05T09:43:13.380913Z"
    },
    "papermill": {
     "duration": 0.012642,
     "end_time": "2023-01-09T13:35:53.312876",
     "exception": false,
     "start_time": "2023-01-09T13:35:53.300234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Я не стал заносить в ноутбук весь мой порядок действий (имеется в \n",
    "виду код, который выполнялся) по поиску гиперпараметров модели, потому что подбор \n",
    "проводился вручную (что наверное глупо...), но подбор паарметров происходил обдуманно и как только получал какой-либо результат сразу же \n",
    "переписывал функции, где менял диапазоны значений параметров. Но здесь напишу словами как производился подбор гиперпараметров.\n",
    "\n",
    "1) Я решил использовать light gradient boost machine classifier\n",
    "(LGBMClassifier). В нем множество гиперпараметров. Сначала я попытался получить \n",
    "хоть какую-то модель, поэтому интуитивно взял следующие значения параметров. \n",
    "boosting_type выбрал rf, то есть random forest, т.к. с ним уже сталкивался. num_leaves \n",
    "по умолчанию равно 31, я решил взять немного побольше - 40. max_depth = 3 - глубина \n",
    "леса = 2 это мало почти всегда, поэтому попробовал 3, learning_rate=0.001 - решил\n",
    "выбрать достаточно малую скорость обучения. n_estimators=15000 - часто чем больше, тем \n",
    "лучше (но слишком много тоже плохо). Параметр min_child_samples = 20. objective='binary' - для LGBMClassifier.  В дополнение я добавил еще несколько \n",
    "гиперпараметров: is_unbalance=True - нужно указывать если выбрал objective='binary', \n",
    "max_bin=100 - для борьбы с переобучением, bagging_freq=1 - чтобы на каждой итерации \n",
    "был bagging, попробовал  bagging_fraction=0.5 (взял среднее между минимальным и \n",
    "максимальным) для увеличения скорости обучения и борьбы с переобучением. Остальные \n",
    "гиперпараметры оставил такими, как по умолчанию.\n",
    "В итоге получил score: 0.04409 - что весьма неплохо.\n",
    "\n",
    "2) Далее решил увеличить значения параметра 'max_depth', прогнал для 3, 4, 5, 6 и \n",
    "увидел, что для 4 результат стал лучше во всех смыслах, а начиная с 5 learning score был больше, а валидационный меньше, что является признаком \n",
    "переобучения. Поэтому я далее глубину не выставлял больше 4.\n",
    "\n",
    "3) Попробовал поварьировать значение параметра скорости обучения при прочих неизменных параметрах- в результате качество модели не менялось => зафиксировал для всех последующих запусков кода learning_rate=0.001\n",
    "\n",
    "4) Создал функцию find_best_params, в которой поварьировал параметры \n",
    "'max_depth' = 3 и 4, 'n_estimators'= 15000, 20000,\n",
    "'min_child_samples' = 15, 20, 25, 'num_leaves'=30, 35, 40, 'max_bin'=50, 75, 100\n",
    "Результаты заносил в df. Анализируя данные пришел к выводу о том, что гиперпараметр \n",
    "num_leaves мало влияет на качество модели, поэтому в дальнейшем использовал значение 40. Среди рассмотренных наборов параметров лучшим оказался следующий:\n",
    "max_depth=4, n_estimators=20000, min_child_samples=15, num_leaves=40, max_bin=100.\n",
    "Остальные гиперпараметры остались без изменения как в пункте 1)\n",
    "При таких параметрах  score: 0.04869 - стало лучше.\n",
    "\n",
    "5) Далее зафиксировал все параметры, кроме n_estimators и min_child_samples: обучал модели с разными значениями этих гиперпараметров. В среднем наблюдается, что с ростом числа деревьев в лесу, растет roc_auc, эта метрика также растет с уменьшением min_child_samples (наиболее оптимальное значение min_child_samples=5)\n",
    "\n",
    "6) Затем попытался добавить регуляризацию. Поварьировал параметры reg_alpha и reg_lambda. Лучшее значение roc_auc для валидационных данных получилось при \n",
    "reg_alpha=reg_lambda=0.01\n",
    "В результате score:0.04875 - чуть-чуть улучшился\n",
    "\n",
    "7) Понял, что прогресса почти нет и добавил новые фичи, связанные со временем совершения покупки (утро/день..., пн/вт/ср...) и запустил обучение модели, с параметрами, как в пункте 6).\n",
    "В результате score: 0.04957 - еще лучше\n",
    "\n",
    "8) Затем оставив все гиперпараметры такими же, я уменьшил долю валидационных данных, т. е. test_size уменьшил с 0.3 до 0.2.\n",
    "В итоге score: 0.05036 (пробовал еще больше уменьшить долю валидационных данных, \n",
    "но score уменьшался в таком случае - видимо нужно другие гиперпараметры подбирать)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc5598",
   "metadata": {
    "papermill": {
     "duration": 0.012761,
     "end_time": "2023-01-09T13:35:53.338745",
     "exception": false,
     "start_time": "2023-01-09T13:35:53.325984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Так подбирались гиперпараметры в пунктах 4-5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a80cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:35:53.367173Z",
     "iopub.status.busy": "2023-01-09T13:35:53.366474Z",
     "iopub.status.idle": "2023-01-09T13:35:53.373815Z",
     "shell.execute_reply": "2023-01-09T13:35:53.372734Z"
    },
    "papermill": {
     "duration": 0.024623,
     "end_time": "2023-01-09T13:35:53.376459",
     "exception": false,
     "start_time": "2023-01-09T13:35:53.351836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "# parameters={\n",
    "#     'max_depth': [4], #4\n",
    "#     'n_estimators': range(10000, 25000, 5000), \n",
    "#     'min_child_samples': range(15,25,5), \n",
    "#     'num_leaves': [40], \n",
    "#     'max_bin': range(45, 100, 15), \n",
    "# }\n",
    "\n",
    "# #создание классификатора с заданными параметрами\n",
    "# def classificator(max_depth, n_estimators, min_child_samples, num_leaves, max_bin):\n",
    "#     clf = LGBMClassifier(\n",
    "#         boosting_type='rf', # ‘gbdt’, ‘dart’, ‘goss’\n",
    "#         n_estimators=n_estimators,\n",
    "#         num_leaves=num_leaves,\n",
    "#         max_depth=max_depth,\n",
    "#         learning_rate=0.001,\n",
    "#         random_state=RANDOM_STATE,\n",
    "#         bagging_freq=1,\n",
    "#         bagging_fraction=0.5,\n",
    "#         importance_type='split',\n",
    "#         is_unbalance=True,\n",
    "#         min_child_samples=min_child_samples,\n",
    "#         min_child_weight=0.001,\n",
    "#         min_split_gain=0.0,\n",
    "#         objective='binary',\n",
    "#         reg_alpha=0.0,\n",
    "#         reg_lambda=0.0,\n",
    "#         verbose=-1,\n",
    "#         subsample=1.0,\n",
    "#         subsample_freq=0,\n",
    "#         max_bin=max_bin           \n",
    "#     )\n",
    "#     return clf\n",
    "\n",
    "\n",
    "# #поиск score на валидационных и тренировочных данных для данного классификатора\n",
    "# def find_scores(max_depth, n_estimators, min_child_samples, num_leaves, max_bin):\n",
    "#     clf_0=classificator(max_depth, n_estimators, min_child_samples, num_leaves, max_bin)\n",
    "#     print('fitting...')\n",
    "#     clf = uplift_fit(clf_0, X_learn, treatment_learn, target_learn) #обучение\n",
    "#     print('successful!')\n",
    "#     learn_pred = uplift_predict(clf, X_learn)\n",
    "#     learn_scores = uplift_metrics(learn_pred, treatment_learn, target_learn)\n",
    "#     valid_pred = uplift_predict(clf, X_valid)\n",
    "#     valid_scores = uplift_metrics(valid_pred, treatment_valid, target_valid)\n",
    "#     print('scores recieved')\n",
    "#     return learn_scores, valid_scores\n",
    " \n",
    "# #занесение результатов в таблицу\n",
    "# answers = pd.DataFrame({\n",
    "#     'max_depth': [],\n",
    "#     'n_estimators': [],\n",
    "#     'min_child_samples': [],\n",
    "#     'num_leaves': [],\n",
    "#     'max_bin': [],\n",
    "#     'learn_roc_auc': [],\n",
    "#     'learn_uplift': [],\n",
    "#     'valid_roc_auc': [],\n",
    "#     'valid_uplift': [],\n",
    "# })\n",
    "# def find_best_params(answers):\n",
    "#     step_num=0 #какая по счету строчка таблицы answers будет заполняться\n",
    "#     depth=4\n",
    "#     num_leaves=40\n",
    "# #     min_child_samples=15\n",
    "# #     max_bin=100\n",
    "#     for n_estimators in parameters['n_estimators']:\n",
    "#         for min_child_samples in parameters['min_child_samples']:\n",
    "#             for max_bin in parameters['max_bin']:\n",
    "#                 step_num=step_num+1\n",
    "#                 print('step_num', step_num, 'out of', 72)\n",
    "#                 learn_scores, valid_scores=find_scores(depth, n_estimators, \\\n",
    "#                                     min_child_samples, num_leaves, max_bin)\n",
    "#                 learn_roc_auc, learn_uplift = learn_scores['roc_auc'], \\\n",
    "#                                               learn_scores['uplift']\n",
    "#                 valid_roc_auc, valid_uplift = valid_scores['roc_auc'], \\\n",
    "#                                               valid_scores['uplift']\n",
    "#                 #добавление строки с результатами\n",
    "#                 answers.loc[len(answers.index)]=[depth, n_estimators, \\\n",
    "#                         min_child_samples,num_leaves, max_bin, learn_roc_auc,\\\n",
    "#                         learn_uplift, valid_roc_auc, valid_uplift]\n",
    "#                 #вывод текущей строки\n",
    "#                 print([depth, n_estimators, min_child_samples, num_leaves, \\\n",
    "#                        max_bin, learn_roc_auc, learn_uplift, valid_roc_auc, \\\n",
    "#                        valid_uplift])\n",
    "\n",
    "#     return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea8d78",
   "metadata": {
    "papermill": {
     "duration": 0.013471,
     "end_time": "2023-01-09T13:35:53.403001",
     "exception": false,
     "start_time": "2023-01-09T13:35:53.389530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Так подбирались гиперпараметры в 6 пункте**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04168a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:35:53.432463Z",
     "iopub.status.busy": "2023-01-09T13:35:53.431740Z",
     "iopub.status.idle": "2023-01-09T13:35:54.292284Z",
     "shell.execute_reply": "2023-01-09T13:35:54.291055Z"
    },
    "papermill": {
     "duration": 0.87905,
     "end_time": "2023-01-09T13:35:54.295232",
     "exception": false,
     "start_time": "2023-01-09T13:35:53.416182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#подбор параметров для регуляризации\n",
    "from lightgbm import LGBMClassifier\n",
    "parameters={\n",
    "    'reg_alpha': [0.0, 0.005, 0.01, 0.05, 0.1, 0.2], #6 штук\n",
    "    'reg_lambda': [0.0, 0.005, 0.01, 0.05, 0.1, 0.2], #6 штук\n",
    "}\n",
    "\n",
    "\n",
    "#создание классификатора с заданными параметрами\n",
    "def classificator(reg_alpha, reg_lambda):\n",
    "    clf = LGBMClassifier(\n",
    "        boosting_type='rf', # ‘gbdt’, ‘dart’, ‘goss’\n",
    "        n_estimators=20000,\n",
    "        num_leaves=40,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.001,\n",
    "        random_state=RANDOM_STATE,\n",
    "        bagging_freq=1,\n",
    "        bagging_fraction=0.5,\n",
    "        importance_type='split',\n",
    "        is_unbalance=True,\n",
    "        min_child_samples=5,\n",
    "        min_child_weight=0.001,\n",
    "        min_split_gain=0.0,\n",
    "        objective='binary',\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        verbose=-1,\n",
    "        subsample=1.0,\n",
    "        subsample_freq=0,\n",
    "        max_bin=100           \n",
    "    )\n",
    "    return clf\n",
    "\n",
    "\n",
    "#поиск score на валидационных и тренировочных данных для данного классификатора\n",
    "def find_scores(reg_alpha, reg_lambda):\n",
    "    clf_0=classificator(reg_alpha, reg_lambda)\n",
    "    print('fitting...')\n",
    "    clf = uplift_fit(clf_0, X_learn, treatment_learn, target_learn) #обучение\n",
    "    print('successful!')\n",
    "    learn_pred = uplift_predict(clf, X_learn)\n",
    "    learn_scores = uplift_metrics(learn_pred, treatment_learn, target_learn)\n",
    "    valid_pred = uplift_predict(clf, X_valid)\n",
    "    valid_scores = uplift_metrics(valid_pred, treatment_valid, target_valid)\n",
    "    print('scores recieved')\n",
    "    return learn_scores, valid_scores\n",
    " \n",
    "#занесение результатов в таблицу\n",
    "answers = pd.DataFrame({\n",
    "    'reg_alpha': [], \n",
    "    'reg_lambda': [],\n",
    "    'learn_roc_auc': [],\n",
    "    'learn_uplift': [],\n",
    "    'valid_roc_auc': [],\n",
    "    'valid_uplift': [],\n",
    "})\n",
    "def find_best_params(answers):\n",
    "    step_num=0 #какая по счету строчка таблицы answers будет заполняться\n",
    "#     depth=4\n",
    "#     num_leaves=40\n",
    "#     max_bin=100\n",
    "#     min_child_samples=5\n",
    "\n",
    "    for reg_alpha in parameters['reg_alpha']:\n",
    "        for reg_lambda in parameters['reg_lambda']:\n",
    "            step_num=step_num+1\n",
    "            print('step_num', step_num, 'out of', 36)\n",
    "            learn_scores, valid_scores=find_scores(reg_alpha, reg_lambda)\n",
    "            learn_roc_auc, learn_uplift = learn_scores['roc_auc'], \\\n",
    "                                          learn_scores['uplift']\n",
    "            valid_roc_auc, valid_uplift = valid_scores['roc_auc'], \\\n",
    "                                          valid_scores['uplift']\n",
    "            #добавление строки с результатами\n",
    "            answers.loc[len(answers.index)]=[reg_alpha, reg_lambda, learn_roc_auc,\\\n",
    "                    learn_uplift, valid_roc_auc, valid_uplift]\n",
    "            #вывод текущей строки\n",
    "            print([reg_alpha, reg_lambda, learn_roc_auc, learn_uplift, valid_roc_auc,\\\n",
    "                   valid_uplift])\n",
    "\n",
    "    return answers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef3016",
   "metadata": {
    "papermill": {
     "duration": 0.013074,
     "end_time": "2023-01-09T13:35:54.321762",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.308688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Сохранение результатов для различных гиперпараметров (закомментировал, т.к. несколько часов вызывается). Чуть ниже применяется классификатор с лучшими значениями гиперпараметров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f841397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:35:54.350168Z",
     "iopub.status.busy": "2023-01-09T13:35:54.349725Z",
     "iopub.status.idle": "2023-01-09T13:35:54.354621Z",
     "shell.execute_reply": "2023-01-09T13:35:54.353517Z"
    },
    "papermill": {
     "duration": 0.022021,
     "end_time": "2023-01-09T13:35:54.357051",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.335030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results=find_best_params(answers)\n",
    "# results.to_csv(\"find_best_clf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a59ab",
   "metadata": {
    "papermill": {
     "duration": 0.01282,
     "end_time": "2023-01-09T13:35:54.383229",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.370409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Поиск лучших гиперпараметров (используются результаты предыдущей ячейки)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62ab12",
   "metadata": {
    "papermill": {
     "duration": 0.012933,
     "end_time": "2023-01-09T13:35:54.409456",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.396523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "В ячейке ниже выбирались отслеживались лучшие значения гиперпараметров для получения max значения 'learn_roc_auc'/'learn_uplift'/'valid_roc_auc'/ 'valid_uplift' \n",
    "(но очевидно, что больше всего интересует случай с максимальным значением 'valid_roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902ccfc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:35:54.437590Z",
     "iopub.status.busy": "2023-01-09T13:35:54.437146Z",
     "iopub.status.idle": "2023-01-09T13:35:54.442340Z",
     "shell.execute_reply": "2023-01-09T13:35:54.441516Z"
    },
    "papermill": {
     "duration": 0.022029,
     "end_time": "2023-01-09T13:35:54.444555",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.422526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results= pd.read_csv('./find_best_clf.csv')\n",
    "# TYPES_OF_BEST=['learn_roc_auc', 'learn_uplift', 'valid_roc_auc', 'valid_uplift']\n",
    "\n",
    "# #поиск строк где лучшее (с выбором какой score нужен для поиска лучшего показателя)\n",
    "# def find_best_params(type_of_best, df):\n",
    "#     name=type_of_best\n",
    "#     max_value=df[name].max()\n",
    "#     indices_where_max_value=df[df[name] == max_value].index\n",
    "#     return df.iloc[indices_where_max_value]\n",
    "    \n",
    "# #вывод всех лучших показателей\n",
    "# def print_all_best_results(TYPES_OF_BEST, df):\n",
    "#     for type_of_best in TYPES_OF_BEST:\n",
    "#         print(f'for {type_of_best}:')\n",
    "#         print(find_best_params(type_of_best, df))\n",
    "\n",
    "\n",
    "# print_all_best_results(TYPES_OF_BEST, results)   \n",
    "# #но больше всего интересует случай, с наилучшим значением 'valid_roc_auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc501b4",
   "metadata": {
    "papermill": {
     "duration": 0.012807,
     "end_time": "2023-01-09T13:35:54.470607",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.457800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Применение лучшего классификатора на тестовых данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a8492",
   "metadata": {
    "papermill": {
     "duration": 0.013327,
     "end_time": "2023-01-09T13:35:54.497729",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.484402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Как уже писал выше, лучше всего было, когда reg_alpha=reg_lambda=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71f9594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:35:54.527785Z",
     "iopub.status.busy": "2023-01-09T13:35:54.526555Z",
     "iopub.status.idle": "2023-01-09T13:46:15.394914Z",
     "shell.execute_reply": "2023-01-09T13:46:15.393308Z"
    },
    "papermill": {
     "duration": 620.899418,
     "end_time": "2023-01-09T13:46:15.410874",
     "exception": false,
     "start_time": "2023-01-09T13:35:54.511456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model for learn data set...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Model is ready\n",
      "Learn scores: {'roc_auc': 0.5389854337841111, 'uplift': 0.11175936158456246}\n",
      "Valid scores: {'roc_auc': 0.5238371510230764, 'uplift': 0.0735946099984085}\n"
     ]
    }
   ],
   "source": [
    "clf_=classificator(0.01,0.01)\n",
    "print('Build model for learn data set...')\n",
    "clf = uplift_fit(clf_, X_learn, treatment_learn, target_learn)\n",
    "print('Model is ready')\n",
    "\n",
    "learn_pred = uplift_predict(clf, X_learn)\n",
    "learn_scores = uplift_metrics(learn_pred, treatment_learn, target_learn)\n",
    "print(f'Learn scores: {learn_scores}')\n",
    "valid_pred = uplift_predict(clf, X_valid)\n",
    "valid_scores = uplift_metrics(valid_pred, treatment_valid, target_valid)\n",
    "print(f'Valid scores: {valid_scores}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4e46642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:46:15.442213Z",
     "iopub.status.busy": "2023-01-09T13:46:15.441732Z",
     "iopub.status.idle": "2023-01-09T13:47:30.607415Z",
     "shell.execute_reply": "2023-01-09T13:47:30.606406Z"
    },
    "papermill": {
     "duration": 75.198406,
     "end_time": "2023-01-09T13:47:30.624118",
     "exception": false,
     "start_time": "2023-01-09T13:46:15.425712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission...\n",
      "Submission is ready\n"
     ]
    }
   ],
   "source": [
    "#сохранение результата для отправки\n",
    "test_pred = uplift_predict(clf, X_test, z = True)\n",
    "print('Saving submission...')\n",
    "save_submission(indices_test,test_pred,'my_submission.csv')\n",
    "print('Submission is ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c0016",
   "metadata": {
    "papermill": {
     "duration": 0.013553,
     "end_time": "2023-01-09T13:47:30.652441",
     "exception": false,
     "start_time": "2023-01-09T13:47:30.638888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ближе к концу соревнования у меня возникла мысль о том, чтобы попробовать другие модели пообучать и что может стоить убрать часть признаков, потому что возможно они только портят качество классификатора (он переобучается). И я начал уже отслеживать важность фичей, нооо на этом и остановился..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d3f9e",
   "metadata": {
    "papermill": {
     "duration": 0.013381,
     "end_time": "2023-01-09T13:47:30.679521",
     "exception": false,
     "start_time": "2023-01-09T13:47:30.666140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Важность фичей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f14cb416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T13:47:30.709469Z",
     "iopub.status.busy": "2023-01-09T13:47:30.708420Z",
     "iopub.status.idle": "2023-01-09T13:47:30.733550Z",
     "shell.execute_reply": "2023-01-09T13:47:30.732277Z"
    },
    "papermill": {
     "duration": 0.042846,
     "end_time": "2023-01-09T13:47:30.736264",
     "exception": false,
     "start_time": "2023-01-09T13:47:30.693418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>express_points_received_median</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>price_from_4400_sum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>is_express_points_received_sum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>express_points_received_max</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>price_from_4400_mean</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ratio_count_express_points_spent_to_received</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>express_points_received_sum</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>price_from_1900_sum</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>proportion_count_express_points_received</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>price_from_950_sum</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender_M</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>price_from_490_sum</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ratio_count_express_to_regular_points_spent</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender_U</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>express_points_spent_median</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>orders_interval_min_eps</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>is_regular_points_spent_sum</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>regular_points_spent_median</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>price_from_950_mean</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>level_1_nunique</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    feature_name  importance\n",
       "34                express_points_received_median           0\n",
       "19                           price_from_4400_sum           0\n",
       "48                is_express_points_received_sum           1\n",
       "33                   express_points_received_max           1\n",
       "20                          price_from_4400_mean           4\n",
       "56  ratio_count_express_points_spent_to_received          12\n",
       "32                   express_points_received_sum          19\n",
       "17                           price_from_1900_sum          38\n",
       "53      proportion_count_express_points_received          54\n",
       "15                            price_from_950_sum         124\n",
       "0                                       gender_M         215\n",
       "13                            price_from_490_sum         233\n",
       "54   ratio_count_express_to_regular_points_spent         235\n",
       "2                                       gender_U         243\n",
       "40                   express_points_spent_median         327\n",
       "75                       orders_interval_min_eps         349\n",
       "45                   is_regular_points_spent_sum         354\n",
       "37                   regular_points_spent_median         366\n",
       "16                           price_from_950_mean         389\n",
       "89                               level_1_nunique         412"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "importances =clf.feature_importances_\n",
    "feature_names=clf.feature_name_\n",
    "clf_importances = pd.DataFrame({'feature_name': feature_names, 'importance':importances})\n",
    "clf_importances=clf_importances.sort_values(by='importance', ascending=True).head(20)\n",
    "\n",
    "clf_importances\n",
    "\n",
    "#можно попробовать обучать классификатор после удаления нескольких наиболее \n",
    "#плохих фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a0bc8",
   "metadata": {
    "papermill": {
     "duration": 0.013684,
     "end_time": "2023-01-09T13:47:30.764449",
     "exception": false,
     "start_time": "2023-01-09T13:47:30.750765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 964.403175,
   "end_time": "2023-01-09T13:47:31.705371",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-09T13:31:27.302196",
   "version": "2.3.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
